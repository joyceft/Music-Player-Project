{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of pyspark\n",
    "Anay spark program start with SpartContext, initialized with SparkConf object, \n",
    "SparkConf contains all parameters.\n",
    "getOrCreate will create a new session or use existed session.\n",
    "SparkSession provide uniform enter point for all API. Like Streaming/SQL/hive..now SparkSession used as enter point of DataFrame and DataSet API(combination of SQL+Hive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster('local[*]')\n",
    "sc=SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RDD\n",
    "for a existed list, use sc.parallelize to create RDD. During operating, the elements in list will be automatically partitioned, and sent to differnt machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4, 5]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd\n",
    "#2.Initialized setting by me as 2k, because my laptop is 2-core.\n",
    "rdd.getNumPartitions()\n",
    "#3.How is partition?\n",
    "rdd.glom().collect()\n",
    "'''\n",
    "Carefully use collect(), it will exceed the memory\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-60b01ccb9234>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-60b01ccb9234>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install psutil\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#As suggested by pyspark, install psutil\n",
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'154750654 \\tar \\t10740680 \\t0 \\t胸是软绵绵的 \\tMC马克 \\t76 \\t0 \\t0 \\t 20170303_1_play.log'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.Another way to create RDD\n",
    "#Read file in rdd. Each line is an item, be careful aout the path\n",
    "#1.get work directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "# 'C:\\\\Users\\\\Tianyi Fang\\\\Desktop\\\\case\\\\Untitled Folder'\n",
    "\n",
    "#2.get the file path\n",
    "rdd1 = sc.textFile(cwd +\"\\\\20170303_1_play.log.fn\")\n",
    "#rdd_whole_files = sc.wholeTextFiles(cwd + filename)\n",
    "rdd1\n",
    "\n",
    "#3.first() will return the first item in rdd\n",
    "rdd1.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformation\n",
    "#### 1.Basic operations\n",
    "- map(): do the same operation for every item in RDD\n",
    "- flatmap():do the same operation for every item in RDD, get a list, then put all results in that list into a new list with flat method\n",
    "- filter(): filter rows based on conditions\n",
    "- distinct():\n",
    "- sample():\n",
    "- sortBy():\n",
    "- collect(): transform all item into a py.list, be caucious when dealing with big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "numbersRDD = sc.parallelize(range(1,10+1))\n",
    "print(numbersRDD.collect())\n",
    "mapRDD = numbersRDD.map(lambda x: x**2)\n",
    "print(mapRDD.collect())\n",
    "filterRDD = numbersRDD.filter(lambda x: x%2 ==0)\n",
    "print(filterRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world!', 'I', 'am', 'learning', 'Spark'] 6\n",
      "[['Hello', 'world!'], ['I', 'am', 'learning', 'Spark']] 2\n"
     ]
    }
   ],
   "source": [
    "sentenceRDD = sc.parallelize(['Hello world!', 'I am learning Spark'])\n",
    "wordRDD = sentenceRDD.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "print(wordRDD.collect(), wordRDD.count())\n",
    "wordRDD2 = sentenceRDD.map(lambda sentence: sentence.split(\" \"))\n",
    "print(wordRDD2.collect(), wordRDD2.count())\n",
    "'''\n",
    "Here flatMap represents operations in Python as:\n",
    "l = ['Hello world!', 'I am learning Spark']\n",
    "ll = []\n",
    "for sentence in l:\n",
    "    ll = ll + sentence.split(\" \")\n",
    "ll\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Use in series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 14, 8, 18]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doubleIfOdd(x):\n",
    "    if x%2 ==1:\n",
    "        return 2*x\n",
    "    else:\n",
    "        return x\n",
    "resultRDD = (numbersRDD\n",
    "            .map(doubleIfOdd)\n",
    "            .filter(lambda x: x >6)\n",
    "            .distinct())\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Paired RDDs-tuple(k-v)\n",
    "- reduceByKey(): excute reduce for all items with same key\n",
    "- groupByKey(): return RDD with tuple(key, listOfValues)\n",
    "- sortByKey(): \n",
    "- countByKey(): \n",
    "- collectAsMap(): same as collect, BUT return dictionary of k-v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 1), ('york', 2), ('says', 1), ('hello', 4)]\n",
      "{'new': 1, 'york': 2, 'says': 1, 'hello': 4}\n",
      "[('hello', 4), ('york', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize([\"Hello hello\", \"Hello New York\", \"York says hello\"])\n",
    "resultRDD=(rdd\n",
    "    .flatMap(lambda sentence:sentence.split(\" \"))  \n",
    "    .map(lambda word:word.lower())                 \n",
    "    .map(lambda word:(word, 1))   #return(word, its count)                  \n",
    "    .reduceByKey(lambda x, y: x + y))  #for the items under same key, add them\n",
    "print(resultRDD.collect())\n",
    "print(resultRDD.collectAsMap())  #collectAsMap类似collect,以k-v字典的形式返回\n",
    "\n",
    "resultRDD.sortByKey(ascending=True).take(2) \n",
    "\n",
    "\n",
    "#find top 2 word with most frequency\n",
    "print(resultRDD\n",
    "      .sortBy(lambda x: x[1], ascending=False)\n",
    "      .take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations between RDDs\n",
    "#### 1.RDD1*RDD2 => RDD3\n",
    "- rdd1.union(rdd2): rdd1 ∪ rdd2 return all items(rows)\n",
    "- rdd1.intersection(rdd2): rdd1 ∩ rdd2\n",
    "- rdd1.substract(rdd2): rdd1 - rdd1 ∩ rdd2\n",
    "- rdd1.cartesian(rdd2): rdd1 和 rdd2中所有的元素笛卡尔乘积（正交和）\n",
    "\n",
    "#### 2. SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD1 Home of different people\n",
    "homesRDD = sc.parallelize([\n",
    "        ('Brussels', 'John'),\n",
    "        ('Brussels', 'Jack'),\n",
    "        ('Leuven', 'Jane'),\n",
    "        ('Antwerp', 'Jill'),\n",
    "    ])\n",
    "\n",
    "#RDD2 Quality of life index for various cities\n",
    "lifeQualityRDD = sc.parallelize([\n",
    "        ('Brussels', 10),\n",
    "        ('Antwerp', 7),\n",
    "        ('RestOfFlanders', 5),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brussels', ('John', 10)),\n",
       " ('Brussels', ('Jack', 10)),\n",
       " ('Antwerp', ('Jill', 7))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homesRDD.join(lifeQualityRDD).collect()   #join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brussels', ('John', 10)),\n",
       " ('Brussels', ('Jack', 10)),\n",
       " ('Antwerp', ('Jill', 7)),\n",
       " ('Leuven', ('Jane', None))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homesRDD.leftOuterJoin(lifeQualityRDD).collect()   #leftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Brussels', (<pyspark.resultiterable.ResultIterable object at 0x03055330>, <pyspark.resultiterable.ResultIterable object at 0x03055170>)), ('Antwerp', (<pyspark.resultiterable.ResultIterable object at 0x030554B0>, <pyspark.resultiterable.ResultIterable object at 0x03055370>)), ('RestOfFlanders', (<pyspark.resultiterable.ResultIterable object at 0x03055450>, <pyspark.resultiterable.ResultIterable object at 0x030554D0>)), ('Leuven', (<pyspark.resultiterable.ResultIterable object at 0x030555B0>, <pyspark.resultiterable.ResultIterable object at 0x03055570>))]\n",
      "[('Brussels', (['John', 'Jack'], [10])), ('Antwerp', (['Jill'], [7])), ('RestOfFlanders', ([], [5])), ('Leuven', (['Jane'], []))]\n"
     ]
    }
   ],
   "source": [
    "print(homesRDD.cogroup(lifeQualityRDD).collect())   #cogroup\n",
    "'''\n",
    "that is hard to read\n",
    "'''\n",
    "print(homesRDD\n",
    ".cogroup(lifeQualityRDD)\n",
    ".map(lambda x:(x[0], (list(x[1][0]), list(x[1][1]))))\n",
    ".collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "#### Spark is always lazy about excution. It will not immediately excute your order, but wait unitl the results are needed(see action), since there may be a series of transformation. In that case, interval storage and deliery are not needed.\n",
    "#### Common Actions\n",
    "- collect(): calculate all items and return all results to driver end，then collect() show results in Python list\n",
    "- first(): return first item in Py.list\n",
    "- take(n): return first items in Py.list\n",
    "- count(): count # of items in RDD\n",
    "- top(n): return first item in Py.list, ordered\n",
    "- reduce(): aggregation for items :aggregate in each parition, then do it in whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1,10+1))\n",
    "rdd.reduce(lambda x, y: x + y)  #reduce(): 对RDD中的items做聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cache()\n",
    "save interval results temperoraily, save time and space of re-calculating, which is especially important for many ML algorithms, since they always process iterations during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5\n"
     ]
    }
   ],
   "source": [
    "#be sure to install numpy first\n",
    "import numpy as np\n",
    "numbersRDD = sc.parallelize(np.linspace(1.0, 10.0, 10))\n",
    "squaresRDD = numbersRDD.map(lambda x: x**2)\n",
    "\n",
    "squaresRDD.cache()  # Preserve the actual items of this RDD in memory\n",
    "\n",
    "avg = squaresRDD.reduce(lambda x, y: x + y) / squaresRDD.count()\n",
    "print(avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
