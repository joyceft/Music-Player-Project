{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of pyspark\n",
    "Anay spark program start with SpartContext, initialized with SparkConf object, \n",
    "SparkConf contains all parameters.\n",
    "getOrCreate will create a new session or use existed session.\n",
    "SparkSession provide uniform enter point for all API. Like Streaming/SQL/hive..now SparkSession used as enter point of DataFrame and DataSet API(combination of SQL+Hive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster('local[*]')\n",
    "sc=SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize RDD\n",
    "for a existed list, use sc.parallelize to create RDD. During operating, the elements in list will be automatically partitioned, and sent to differnt machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4, 5]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd\n",
    "#2.Initialized setting by me as 2k, because my laptop is 2-core.\n",
    "rdd.getNumPartitions()\n",
    "#3.How is partition?\n",
    "rdd.glom().collect()\n",
    "'''\n",
    "Carefully use collect(), it will exceed the memory\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-60b01ccb9234>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-60b01ccb9234>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install psutil\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#As suggested by pyspark, install psutil\n",
    "pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'154750654 \\tar \\t10740680 \\t0 \\t胸是软绵绵的 \\tMC马克 \\t76 \\t0 \\t0 \\t 20170303_1_play.log'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.Another way to create RDD\n",
    "#Read file in rdd. Each line is an item, be careful aout the path\n",
    "#1.get work directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "# 'C:\\\\Users\\\\Tianyi Fang\\\\Desktop\\\\case\\\\Untitled Folder'\n",
    "\n",
    "#2.get the file path\n",
    "rdd1 = sc.textFile(cwd +\"\\\\20170303_1_play.log.fn\")\n",
    "#rdd_whole_files = sc.wholeTextFiles(cwd + filename)\n",
    "rdd1\n",
    "\n",
    "#3.first() will return the first item in rdd\n",
    "rdd1.first()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Transformation\n",
    "1.Basic operations\n",
    "- map(): do the same operation for every item in RDD\n",
    "- flatmap():do the same operation for every item in RDD, get a list, then put all results in that list into a new list with flat method\n",
    "- filter(): filter rows based on conditions\n",
    "- distinct():\n",
    "- sample():\n",
    "- sortBy():\n",
    "- collect(): transform all item into a py.list, be caucious when dealing with big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "numbersRDD = sc.parallelize(range(1,10+1))\n",
    "print(numbersRDD.collect())\n",
    "mapRDD = numbersRDD.map(lambda x: x**2)\n",
    "print(mapRDD.collect())\n",
    "filterRDD = numbersRDD.filter(lambda x: x%2 ==0)\n",
    "print(filterRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world!', 'I', 'am', 'learning', 'Spark'] 6\n",
      "[['Hello', 'world!'], ['I', 'am', 'learning', 'Spark']] 2\n"
     ]
    }
   ],
   "source": [
    "sentenceRDD = sc.parallelize(['Hello world!', 'I am learning Spark'])\n",
    "wordRDD = sentenceRDD.flatMap(lambda sentence: sentence.split(\" \"))\n",
    "print(wordRDD.collect(), wordRDD.count())\n",
    "wordRDD2 = sentenceRDD.map(lambda sentence: sentence.split(\" \"))\n",
    "print(wordRDD2.collect(), wordRDD2.count())\n",
    "'''\n",
    "Here flatMap represents operations in Python as:\n",
    "l = ['Hello world!', 'I am learning Spark']\n",
    "ll = []\n",
    "for sentence in l:\n",
    "    ll = ll + sentence.split(\" \")\n",
    "ll\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Use in series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 14, 8, 18]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doubleIfOdd(x):\n",
    "    if x%2 ==1:\n",
    "        return 2*x\n",
    "    else:\n",
    "        return x\n",
    "resultRDD = (numbersRDD\n",
    "            .map(doubleIfOdd)\n",
    "            .filter(lambda x: x >6)\n",
    "            .distinct())\n",
    "resultRDD.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
